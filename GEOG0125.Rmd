--- 
title: "GEOG0125: Advanced Topics in Social Data Science"
author: Justin van Dijk^[Department of Geography, https://www.mappingdutchman.com/] and Stephen Law^[Department of Geography, https://www.geog.ucl.ac.uk/people/academic-staff/stephen-law]
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
link-citations: yes
github-repo: "jtvandijk/GEOG0125"
description: "GEOG0125: Advanced Topics in Social Data Science."
url: 'https\://jtvandijk.github.io/GEOG0125/'
---

# Welcome {-}

Welcome to **Advanced Topics in Social and Geographic Data Science**, one of the elective modules for the MSc in Geographic and Social Data Science here at [UCL Geography](https://www.geog.ucl.ac.uk/).This module has been designed as an advanced topics module to learn data science concepts and methods and to apply them in the domains of social science and geography. 

The first half of the module introduce concepts such as statistical inference, deep learning and convolutional neural network which will end with a keynote lecture from Carto on Spatial Data Science. In the second half of the module, we will introduce you to the practical skills of being a data scientists including learning about database design, unix-tools and web visualisation. We hope this module can provide you with the necessary knowledge in becoming a social and geographic data scientist in the future.

## Weekly topics {-}
| Week          | Date          | Topic | Moodle | GitHub |
| :------ |:-----|:--------------| :--: | :--: |
| 1             | 11/01/2021    | Introduction to AdvSocGeo Data Science | **x** | |
| 2             | 18/01/2021    | [Statistical Inference and Causality](statistical-inference-and-causality.html) | | **x** |
| 3             | 25/01/2021    | GeoAI: Introduction to Deep Learning | **x** | |
| 4             | 01/02/2021    | GeoAI: Convolutional Neural Networks | **x** | |
| 5             | 08/02/2021    | Keynote: Spatial Data Science (Carto) | **x** | |
| *reading week*  | *reading week*  | *reading week* | | |
| 6             | 22/02/2021    | GeoAI: Spatial Data Science Applications | **x** | |
| 7             | 01/03/2021    | DS Tools: [Spatial-Temporal Mobility Analysis](spatial-temporal-mobility-analysis.html) | | **x** |
| 8             | 08/03/2021    | DS Tools: [Unix tools and Database Management](unix-tools-and-database-management.html) | | **x** |
| 9             | 15/03/2021    | DS Tools: [Web Visualisation](web-visualisation.html) | | **x** |
| 10            | 22/03/2021    | Summary and Future of SocGeoDataScience | **x** | |

## Moodle {-}
All important information and communication in relation to this module will be provided on [Moodle](https://moodle.ucl.ac.uk/course/view.php?id=23879). Even though some of the short lecture videos and practical materials will be hosted on this webpage, do check on Moodle regularly to see if there are any updates or important messages.

## Useful additional resources {-}
Besides the mandatory and recommended reading for this course, there are some additional resources that are worth checking out:

1. MIT's introduction course on mastering the command line: [The Missing Semester of Your CS Education](https://missing.csail.mit.edu/) 

2. A useful tool to unpack command line instructions: [explainshell.com](https://www.explainshell.com/)

3. Online resource to develop and check your regular expressions: [regexr.com](https://regexr.com/5jv0v)

4. Selecting colour palettes for your map making and data visualisation: [colorbrewer 2.0](https://colorbrewer2.org)

## Office hours {-}
Office hours with Dr Justin van Dijk (Tuesdays between 15h00 and 16h00) can be booked [here](https://outlook.office365.com/owa/calendar/JustinvanDijkBookableOfficeHours@ucl.ac.uk/bookings/). 

<!--chapter:end:index.Rmd-->

# Statistical Inference and Causality

## Introduction
Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. In other words: statistical inference is the procedure through which we try to make inferences about a population based on characteristics of this population that have been captured in a sample. This includes uncovering the association between variables as well as establishing causal relationships. However, correlation does not imply causation and identifying causal relationships from observational data is not trivial.

> "Correlation does not imply causation" (Any statistician).

This week we will dive a little deeper into statistical inference and how to establish causal relationships by taking a small dive into the field of study of [econometrics](https://en.wikipedia.org/wiki/Econometrics#:~:text=Econometrics%20is%20the%20application%20of,by%20appropriate%20methods%20of%20inference%22.). This week is structured by **4** short lecture videos, reading material, a tutorial in R with a 'hands-on' application of the techniques covered in the lecture videos, and a seminar on Tuesday. The short lecture videos this week are provided by an absolute expert in the field of econometrics: [Dr Idahosa](https://www.uj.ac.za/contact/Pages/Love-Idahosa.aspx) from the [University of Johannesburg](https://www.uj.ac.za/).

Let's get to it!

#### Video: Introduction W01 {-}
```{r 01-short-lecture-welcome, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
embed_msstream('') %>% use_align('left')
```
[Lecture slides] [[Watch on MS stream]]()

```{r 01-settings, warnings=FALSE, message=FALSE, echo=FALSE}
# settings
options(max.print=30)
```

### Reading list {#reading-list-w01}
Please find the reading list for this week below.

#### Core reading {-}
- Bzdok, D., Altman, and M. Krzywinski. 2018. Statistics versus machine learning. *Nature Methods* 15: 233-234. [[Link]](https://www.nature.com/articles/nmeth.4642)
- Idahosa, L. O., Marwa, N., and J. Akotey. 2017. Energy (electricity) consumption in South African hotels: A panel data analysis. *Energy and Buildings* 156: 207-217. [[Link]](https://ucl-new-primo.hosted.exlibrisgroup.com/permalink/f/1klfcc3/TN_cdi_gale_infotracacademiconefile_A523007644)
- Stock, J. and M. Watson. 2019. **Chapter 1**: *Economic Questions and Data*. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.43-54. Harlow: Pearson Education Ltd. [[Link]](https://ucl-new-primo.hosted.exlibrisgroup.com/permalink/f/1klfcc3/TN_cdi_proquest_ebookcentral_EBC6399072)
- Stock, J. and M. Watson. 2019. **Chapter 10**: *Regression with Panel Data*. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.362-382. Harlow: Pearson Education Ltd. [[Link]](https://ucl-new-primo.hosted.exlibrisgroup.com/permalink/f/1klfcc3/TN_cdi_proquest_ebookcentral_EBC6399072)

#### Supplementary reading {-}
- Idahosa, L. O. and J. T. van Dijk. 2016. South Africa: Freedom for whom? Inequality, unemployment and the elderly. *Development* 58(1): 96-102. [[Link]](https://ucl-new-primo.hosted.exlibrisgroup.com/permalink/f/1klfcc3/TN_cdi_proquest_journals_1758956430)

### Technical Help session
Every Thursday between 12h00-13h00 you can join the **Technical Help** session on Microsoft Teams. The session will be hosted by [Alfie](https://www.ucl.ac.uk/geospatial-analytics/people/alfie-long). He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you are having in the Technical Help channel on the GEOG0125 Team so that Alfie will can help to find a solution. 

## Statistical inference
Where during the remainder of this module we will predominantly focus on different [machine learning](https://en.wikipedia.org/wiki/Machine_learning#:~:text=Machine%20learning%20(ML)%20is%20the,a%20subset%20of%20artificial%20intelligence.) methods and techniques) and the data scientist's toolbox, this week we will focus on statistical inference. Although there is considerable overlap sometimes even involving the same methods, the major difference between machine learning and statistics is their purpose. Machine learning models are designed to make **accurate predictions**. Statistical models are designed for inference about the **relationships between variables**. 

>"Statistics draws population inferences from a sample, and machine learning finds generalizable
predictive patterns" (Bzdok *et al.* 2018).

Before we move onto the more complicated material, let's have a quick refresher on linear regression models. Linear regression models offer a very **interpretable** way to model the association between variables. A linear regression model is used to find the line that minimises the mean squared error accross all data to establish the relationship between a response variable one or more explanatory (independent) variables. The case of one explanatory variable is called a **simple linear regression**; if more explanatory variables are used it is called **multiple regression**.

#### Video: Linear regression models {-}
```{r 02-video-linear-models, warnings=FALSE, message=FALSE, echo=FALSE}
library(vembedr)
embed_msstream('') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()

## Causality
### What is causality?
It should now be clear that regression is a statistical technique with the purpose to predict a target variable $y$ according to some other variable $x$ or variables $x_{1}$, $x_{2}$, etc. Regression, however, cares about correlation: what happens to $Y$ when $X=x$. However, correlation does not imply causation. A great example is the relationships between [chocolate consumption and the probability of winning a Nobel price](https://www.reuters.com/article/us-eat-chocolate-win-the-nobel-prize/eat-chocolate-win-the-nobel-prize-idUSBRE8991MS20121010): The higher a countryâ€™s chocolate consumption, the more Nobel laureates it spawns per capita. There is a clear correlation, but there should not exist an actual causal relationship between these two variables (Well, some research suggest it still remains [unclear whether the correlation is spurious](https://doi.org/10.1016/j.ssaho.2020.100082) or an indication for hidden variables ...)

The golden standard to establish a causal relationship is to set-up and execute a [randomised control trial](https://adc.bmj.com/content/90/8/840), think of the many large-scale randomised control trials that are currently taking place to test the safety and effectiveness of various candidate coronavirus vaccines. However, it is not always possible to set up a randomised control trial. Sometimes this has to do with the nature of the relationship being investigated (e.g. establishing the effects of policy changes), but there could also be financial and ethical constraints. As an alternative, one could try to identify causal relationship from observational data. This is known as causal inference and most research in econometrics is concerned with retrieving valid estimates using different regression methods.

<div class="note">
The distinction between causal and non-causal relationships is crucial and heavily depends on your research questions. If you are trying to classify Google Street view images and predict whether the photo contains an office building or a residential building, you want to create a model that predicts this as good as possible. However, you do not care of what was the cause of the building in the photo to be an office building or a residential building. That being said: almost any question is causal and where statistics is used in almost any field of inquiry, few pay proper attention to understanding causality.
</div>

By now you may wonder, sure, but what then is causality? We can say that $X$ causes $Y$ if we were to change the value of $X$ without changing anything else then as a result $Y$ would also change. A simple example: if you switch on the light switch, your light will go on. The action of flipping the light switch causes your light to go on. This being said: it does not mean that $X$ is necessarily the only thing that causes $Y$ (e.g. the light bulb is burnt out or the light was already on) and perhaps a better way of phrasing it is to say that there is a causal relationship between variables if $X$ changes the probabiblity of $Y$ happening or changing.

### The problem with causality
The problem with establishing a causal relationship is that in many cases you cannot 'switch on' or 'switch off' a characteristic. Let's go through this by thinking whether some $X$ causes $Y$. Our $X$ is coded as 0 or 1, for instance, 0 if a person has not received a coronavirus vaccination and 1 if a person has received a coronavirus vaccination. $Y$ is some numeric value. So how do we check if $X$ causes $Y$? What we would need to do for everyone in our sample is to check what will happen to $Y$ when we make $X=0$ and when we make $X=1$. But this is obviously a problem! You cannot both have $X=0$ and $X=1$: you either got inoculated against the coronavirus or you did not. This means that if $X=1$ you can measure what the value of $Y$ is, but you do not know what the value of $Y$ **would have been** if $X=0$. A solution you may come up with is to compare $Y$ between individuals who have $X=0$ and $X=1$. However, there is another problem: there could be all kinds of reasons on why $Y$ differs between individuals that are not necessarily related to $X$. 

<div class="note">
This section heavily borrows material and explanations from Nick Huntington-Klein's excellent [ECON 305: Economics, Causality, and Analytics](http://www.nickchk.com/econ305.html) module, do have a look if you want to learn more about this topic.
</div>

This brings us to econometrics and causal inference: the main goal of causal inference is to make the best possible estimation of what $Y$ would have been if $X$ would have been different, the so-called **counterfactual**. As we cannot always use an experiment in which we can randomly assign $X$ so that we know that on average people with $X=1$ and the same as people with $X=2$, we have to come up with a model to figure out what the counterfactual would do. In the following, we will explore two ways of doing this through so-called fixed effect models and random effect models in the situation in which we have data points for each observation across time (i.e. [longitudinal data](https://en.wikipedia.org/wiki/Longitudinal_study) or [panel data](https://en.wikipedia.org/wiki/Panel_data)).

###Fixed effect models
Fixed effects are variables that are constant across individuals; these variables, like age, sex, or ethnicity, typically do not change over time or change over time at a constant rate. As such, they have fixed effects on a dependent variable $Y$. As such, using a fixed effect model you can explore the relationship between variables within an entity (which could be persons, companies, countries, etc.). Each entity has its own individual charactersitics that may or may not influence the dependent variable. When using a fixed effect model, we assume that something within the entity may impact or bias the dependent variables and we need to control for this. A fixed effect model does this by removing the characteristics that do not change over time so that we can assess the net effect of the independent variables on the dependent variable.

#### Video: fixed effect models {-}
```{r 02-video-fixed-effect-models, warnings=FALSE, message=FALSE, echo=FALSE}
embed_msstream('') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()

Let's try to to apply a fixed effect model in R. For this we will use a data set containing some [panel data](https://en.wikipedia.org/wiki/Panel_data). The data set contains data, for different countries and years, an undefined variable $Y$ that we want to explain with some other variables.

#### File download {-}
| File                                                 | Type           | Link |
| :------                                              | :------        | :------ |
| Example Panel Data       | `csv`             | [Download](https://github.com/jtvandijk/GEOG0125/tree/master/raw/zip/paneldata.zip) |

```{r 02-load-libraries, warnings=FALSE, message=FALSE}
# load libraries
library(tidyverse)
library(plm)
library(car)
library(gplots)
library(tseries)
library(lmtest)

# read data
country_data <- read_csv('_book/raw/w02/paneldata.csv')

# inspect
head(country_data)
```

Upon inspecting the `dataframe` you can see that the data contains 8 different fictional countries. For each country we have several years of data: three independent variables names $x_{1}$, $x_{2}$, and $x_{3}$ and a dependent variable $y$. As this is a panel dataset we have to declare it as such using the `plm.data()` function from the `plm` package. The `plm` package is a library dedicated to panel data analysis.

```{r 02-create-panel-data, warnings=FALSE, message=FALSE}
# create a panel data object
country_panel <- pdata.frame(country_data, index=c('country','year'))

# inspect
country_panel
```

Although the data looks the same, you can see that the row index has been updated to reflect the `country` and `year` variables. Let's inspect the data using a boxplot as well as a [conditioning plot](https://www.oreilly.com/library/view/graphing-data-with/9781491922606/ch18.html). A coplot is a method for visualising interactions in your data set: it shows you how some variables are conditional on some other set of variables. So, for our panel data set, we can look at the variation of $Y$ over time by country. The bars at top indicate the countries position from left to right starting on the bottom row.

```{r 02-co-plot, warnings=FALSE, message=FALSE}
# create a quick box plot
scatterplot(y ~ year, data=country_panel)

# create a quick conditioning plot
coplot(y ~ year|country, data=country_panel, type='b')
```

The graphs show that there $Y$ is variable both over time and between countries. Let's also have a look at the [heterogeinity](https://en.wikipedia.org/wiki/Homogeneity_and_heterogeneity) of our data by plotting the means, and the 95% confidence interval around the means, of $Y$ accross time and accross countries.

```{r 02-heterogeneity, warnings=FALSE, message=FALSE}
# means accross years
plotmeans(y ~ year, data=country_panel)

# means accross countries
plotmeans(y ~ country, data=country_panel)
```

There is clearly some heterogeneity accross the countries and accross years. However, the basic Ordinary Least Squares (OLS) regression model does not consider this heterogeneity:

```{r 02-run-the-OLS, warnings=FALSE, message=FALSE}
# run an OLS
ols <- lm(y ~ x1, data=country_panel)

# summary
summary(ols)
```

The results show that there is no significant statistical relationship between $x_{1}$ and the dependent variable $Y$ as the model tries to explain all variability in the data at once. You can clearly see this in the plot below:

```{r 02-plot-the-ols, warnings=FALSE, message=FALSE}
# plot the results
scatterplot(country_panel$y ~ country_panel$x1, boxplots=FALSE, smooth=FALSE, pch=19, col='black')

# add the OLS regression line
abline(lm(y ~ x1, data=country_panel),lwd=3, col='red')
```

One way of getting around this, and fixing the effects of the *country* variable, is by using a model incorporating dummy variables through a Least Squares Dummy Variable model (LSDV).

```{r 02-fixed-effects-with-dummies, warnings=FALSE, message=FALSE}
# run a LSDV
fixed_dum <- lm(y ~ x1 + factor(country) - 1, data=country_panel)

# summary
summary(fixed_dum)
```

The least square dummy variable model (LSDV) provides a good way to understand fixed effects. By adding the dummy for each country, we are now estimating the pure effect of $x_{1}$ on $Y$. Each dummy is absorbing the effects that are particular to each country. Where the independent variable $x_{1}$ was not significant in the OLS model, after controlling for differences across countries, $x_{1}$ became significant in the LSDV model. You can clearly see why this is happening in the plot below:

```{r 02-plot-the-lsdv, warnings=FALSE, message=FALSE}
# plot the results
scatterplot(fixed_dum$fitted ~ country_panel$x1|country_panel$country, boxplots=FALSE, smooth=FALSE, col='black')

# add the LSDV regression lines
abline(lm(y ~ x1, data=country_panel),lwd=3, col='red')
```

We can also run a country-specific fixed effect model by using specific intercepts for each country. We can achieve this by using the `plm` package.

```{r 02-fixed-effects-plm, warnings=FALSE, message=FALSE}
# run a fixed effect model
fixed <- plm(y ~ x1, data=country_panel, model='within')

# summary
summary(fixed)
```

Here you can check the individual intercepts through `fixef(fixed`). The coefficient of $x_{1}$ indicates how much $Y$ changes on average over time per country: as you can see the results are the same. Arguably, however, running your model with explicit dummy variables is more informative.

### Random effect models
Random effects are the opposite of fixed effects. Contrary to fixed effects, random effects are random and difficult to predict. As such, the effect they will have on a dependent variable $Y$ is not constant. Think of the cost of renting a one bedroom appartement: [rental prices vary greatly](https://www.ons.gov.uk/economy/inflationandpriceindices/bulletins/indexofprivatehousingrentalprices/may2020#:~:text=Private%20rental%20prices%20paid%20by,12%20months%20to%20May%202020.) depending on location. So if you have reason to believe that these differences across entities have some influence on the dependent variable, then you would use a random effects model approach. Let's have a closer look at dealing with random effects in the context of panel data.

#### Video: random effect models {-}
```{r 02-video-random-effect-models, warnings=FALSE, message=FALSE, echo=FALSE}
embed_msstream('') %>% use_align('left')
```
[[Lecture slides]]() [[Watch on MS stream]]()



## Take home message


## Attributions {#attributions_w02}
This week's content and practical uses content and inspiration from:

- Torres-Reyna, Oscar. 2010. Getting Started in Fixed/Random Effects Models using R. [[Link]](
https://rstudio-pubs-static.s3.amazonaws.com/372492_3e05f38dd3f248e89cdedd317d603b9a.html#45_regression_diagnostics)
- Huntington-Klein, Nick. 2019. ECON 305: Economics, Causality, and Analytics. Lecture 13: Causality. [[Link]](http://www.nickchk.com/econ305.html)

<!--chapter:end:02-week02.Rmd-->

# Spatial-Temporal Mobility Analysis

This week we will be looking at capturing mobility data with GPS, specifically we will look at how to obtain, clean, and interpret a GPS data set. We will further use Machine Learning algorithms, specifically Support Vector Machines and Tree-based methods, to classify labeled GPS data into â€˜stayâ€™ and â€˜moveâ€™ points.

<center>
**This weekâ€™s content will be made available on 01/03/2021.**
</center>

<!--chapter:end:07-week07.Rmd-->

# Unix tools and Database Management

This week we will be looking at the Unix tools and database management. Basic knowledge of the Unix shell as well as knowing how to retrieve datasets from databases are essential tools in a data scientistâ€™s toolkit. This week you will be introduced to both. In the practical component of this week you will work within the 'bourne again shell' (bash) in a command-line environment to create a parallel data processing pipeline involving a database and Unix tools.

<center>
**This weekâ€™s content will be made available on 08/03/2021.**
</center>

<!--chapter:end:08-week08.Rmd-->

# Web Visualisation

Mapping is at the core of Spatial Data Science. This week we will introduce you to 'mapping at scale': how to display large amounts of data with interactive maps through slippy maps. In the practical component we will be using map tiles to create an interactive web map from scratch using `JavaScript` and `Leaflet.js`.

<center>
**This weekâ€™s content will be made available on 15/03/2021.**
</center>

<!--chapter:end:09-week09.Rmd-->

