[
["statistical-inference-and-causality.html", "1 Statistical Inference and Causality 1.1 Introduction 1.2 Statistical inference 1.3 Causality 1.4 Take home message 1.5 Attributions", " 1 Statistical Inference and Causality 1.1 Introduction Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. In other words: statistical inference is the procedure through which we try to make inferences about a population based on characteristics of this population that have been captured in a sample. This includes uncovering the association between variables as well as establishing causal relationships. However, correlation does not imply causation and identifying causal relationships from obsevational data is not trivial. “Correlation does not imply causation” (Any statistician). This week we will dive a little deeper into statistical inference and how to establish causal relationships by taking a small dive into the field of study of econometrics. This week is structured by 4 short lecture videos, reading material, a tutorial in R with a ‘hands-on’ application of the techniques covered in the lecture videos, and a seminar on Tuesday. The short lecture videos this week are provided by an absolute expert in the field of econometrics: Dr Idahosa from the University of Johannesburg. Let’s get to it! Video: Introduction W01 [Lecture slides] [Watch on MS stream] 1.1.1 Reading list Please find the reading list for this week below. Core reading Bzdok, D., Altman, and M. Krzywinski. 2018. Statistics versus machine learning. Nature Methods 15: 233-234. [Link] Idahosa, L. O., Marwa, N., and J. Akotey. 2017. Energy (electricity) consumption in South African hotels: A panel data analysis. Energy and Buildings 156: 207-217. [Link] Stock, J. and M. Watson. 2019. Chapter 1: Economic Questions and Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.43-54. Harlow: Pearson Education Ltd. [Link] Stock, J. and M. Watson. 2019. Chapter 10: Regression with Panel Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.362-382. Harlow: Pearson Education Ltd. [Link] Supplementary reading Idahosa, L. O. and J. T. van Dijk. 2016. South Africa: Freedom for whom? Inequality, unemployment and the elderly. Development 58(1): 96-102. [Link] 1.1.2 Technical Help session Every Thursday between 12h00-13h00 you can join the Technical Help session on Microsoft Teams. The session will be hosted by Alfie. He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you ar having in the Technical Help channel on the GEOG0125 Team so that Alfie will can help to find a solution. 1.2 Statistical inference Where during the remainder of this module we will predominantly focus on different machine learning methods and techniques) and the data scientist’s toolbox, this week we will focus on statistical inference. Although there is considerable overlap sometimes even involving the same methods, the major difference between machine learning and statistics is their purpose. Machine learning models are designed to make accurate predictions. Statistical models are designed for inference about the relationships between variables. “Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns” (Bzdok et al. 2018). Video: Linear regression models Before we move onto the more complicated material, let’s have a quick refresher on linear regression models. Linear regression models offer a very interpretable way to model the association between variables. A linear regression model is used to find the line that minimises the mean squared error accross all data to establish the relationship between a response variable one or more explanatory (independent) variables. The case of one explanatory variable is called a simple linear regression; if more explanatory variables are used it is called multiple regression. [Lecture slides] [Watch on MS stream] 1.3 Causality ake several popular methods for getting causal effects out of non-experimental data and provide animated plots that show you what these methods actually do to the data and how you can understand what the effects you’re estimating actually ARE. Regression is a statistical analysis technique, the purpose is to predict a target variable y according to some other variables x1, x2, …, namely, if you passively “see” that X=x, what the value of Y will be? Regression cares about correlation relationships. You can always get a regression formula between Y and X, even when they by no means entail any causal relationships. However, correlation does not necessary mean causation. An example is that we can build a regression model between “chocolate consumption quantity of a person” and “the probability the person to win the Nobel price” (cf.: http://www.businessinsider.com.au/chocolate-consumption-vs-nobel-prizes-2014-4?r=US&amp;IR=T ) , but there shouldn’t exist actual causal relationship between them. Identifying causal relationships from observational data is not easy. Still, researchers are often interested in examining the effects of policy changes or other decisions. In those analyses, researchers will face any number of analytical decisions, including whether to use fixed or random effects models to control for variables that don’t change over time. Let’s consider an example. Suppose we’re interested in estimating the effect that a government grant might have had on firms’ product quality (as examined in this previous study). In addition to controlling for observed variables like the number of employees the firms had at different time points in the study period, we might also want to control for unobserved variables, such as the management quality of the firms. Assuming that the firms’ management quality is constant over time, we can use regression models to try to account for those unobserved factors — but there isn’t always consensus about the best way to do so. Specifically, researchers often must decide whether to use a fixed or random effects approach in an analysis like this. 1.3.1 Fixed-effect models Video: Fixed-effect models [Lecture slides] [Watch on MS stream] 1.3.2 Random-effect models Video: Random-effect models [Lecture slides] [Watch on MS stream] 1.4 Take home message 1.5 Attributions This week’s content and practical uses content and inspiration from: Torres-Reyna, Oscar. 2010. Getting Started in Fixed/Random Effects Models using R. [Link] Huntington-Klein, Nick. 2019. ECON 305: Economics, Causality, and Analytics. [Link] "]
]
