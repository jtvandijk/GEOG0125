[
["statistical-inference-and-causality.html", "1 Statistical Inference and Causality 1.1 Introduction 1.2 Statistical inference 1.3 Causality 1.4 Take home message 1.5 Attributions", " 1 Statistical Inference and Causality 1.1 Introduction Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. In other words: statistical inference is the procedure through which we try to make inferences about a population based on characteristics of this population that have been captured in a sample. This includes uncovering the association between variables as well as establishing causal relationships. However, correlation does not imply causation and identifying causal relationships from observational data is not trivial. “Correlation does not imply causation” (Any statistician). This week we will dive a little deeper into statistical inference and how to establish causal relationships by taking a small dive into the field of study of econometrics. This week is structured by 4 short lecture videos, reading material, a tutorial in R with a ‘hands-on’ application of the techniques covered in the lecture videos, and a seminar on Tuesday. The short lecture videos this week are provided by an absolute expert in the field of econometrics: Dr Idahosa from the University of Johannesburg. Let’s get to it! Video: Introduction W01 [Lecture slides] [Watch on MS stream] 1.1.1 Reading list Please find the reading list for this week below. Core reading Bzdok, D., Altman, and M. Krzywinski. 2018. Statistics versus machine learning. Nature Methods 15: 233-234. [Link] Idahosa, L. O., Marwa, N., and J. Akotey. 2017. Energy (electricity) consumption in South African hotels: A panel data analysis. Energy and Buildings 156: 207-217. [Link] Stock, J. and M. Watson. 2019. Chapter 1: Economic Questions and Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.43-54. Harlow: Pearson Education Ltd. [Link] Stock, J. and M. Watson. 2019. Chapter 10: Regression with Panel Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.362-382. Harlow: Pearson Education Ltd. [Link] Supplementary reading Idahosa, L. O. and J. T. van Dijk. 2016. South Africa: Freedom for whom? Inequality, unemployment and the elderly. Development 58(1): 96-102. [Link] 1.1.2 Technical Help session Every Thursday between 12h00-13h00 you can join the Technical Help session on Microsoft Teams. The session will be hosted by Alfie. He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you are having in the Technical Help channel on the GEOG0125 Team so that Alfie will can help to find a solution. 1.2 Statistical inference Where during the remainder of this module we will predominantly focus on different machine learning methods and techniques) and the data scientist’s toolbox, this week we will focus on statistical inference. Although there is considerable overlap sometimes even involving the same methods, the major difference between machine learning and statistics is their purpose. Machine learning models are designed to make accurate predictions. Statistical models are designed for inference about the relationships between variables. “Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns” (Bzdok et al. 2018). Before we move onto the more complicated material, let’s have a quick refresher on linear regression models. Linear regression models offer a very interpretable way to model the association between variables. A linear regression model is used to find the line that minimises the mean squared error accross all data to establish the relationship between a response variable one or more explanatory (independent) variables. The case of one explanatory variable is called a simple linear regression; if more explanatory variables are used it is called multiple regression. Video: Linear regression models [Lecture slides] [Watch on MS stream] 1.3 Causality It should now be clear that regression is a statistical technique with the purpose to predict a target variable \\(y\\) according to some other variable \\(x\\) or variables \\(x_{1}\\), \\(x_{2}\\), etc. Regression, however, cares about correlation: what happens to \\(Y\\) when \\(X=x\\). However, correlation does not imply causation. A great example is the relationships between chocolate consumption and the probability of winning a Nobel price: The higher a country’s chocolate consumption, the more Nobel laureates it spawns per capita. There is a clear correlation, but there should not exist an actual causal relationship between these two variables (Well, some research suggest it still remains unclear whether the correlation is spurious or an indication for hidden variables …) The golden standard to establish a causal relationship is to set-up and execute a randomised control trial, think of the many large-scale randomised control trials that are currently taking place to test the safety and effectiveness of various candidate coronavirus vaccines. However, it is not always possible to set up a randomised control trial. Sometimes this has to do with the nature of the relationship being investigated (e.g. establishing the effects of policy changes), but there could also be financial and ethical constraints. As an alternative, one could try to identify causal relationship from observational data. This is known as causal inference and most research in econometrics is concerned with retrieving valid estimates using different regression methods. 1.3.1 Fixed-effect models Video: Fixed-effect models [Lecture slides] [Watch on MS stream] 1.3.2 Random-effect models Video: Random-effect models [Lecture slides] [Watch on MS stream] 1.4 Take home message 1.5 Attributions This week’s content and practical uses content and inspiration from: Torres-Reyna, Oscar. 2010. Getting Started in Fixed/Random Effects Models using R. [Link] Huntington-Klein, Nick. 2019. ECON 305: Economics, Causality, and Analytics. [Link] "]
]
