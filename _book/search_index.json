[
["index.html", "GEOG0125: Advanced Topics in Social and Geographic Data Science Welcome Weekly topics Moodle Useful additional resources Office hours", " GEOG0125: Advanced Topics in Social and Geographic Data Science Justin van Dijk1 and Stephen Law2 2021-02-18 Welcome Welcome to Advanced Topics in Social and Geographic Data Science, one of the elective modules for the MSc in Geographic and Social Data Science here at UCL Geography.This module has been designed as an advanced topics module to learn data science concepts and methods and to apply them in the domains of social science and geography. The first half of the module introduces a variety of important concepts such as statistical inference, deep learning, and convolutional neural networks. We will end with a keynote lecture from Carto on Spatial Data Science. In the second half of the module, we will introduce you to the practical skills of being a data scientists including an introduction to databases, unix-tools and web visualisation (a data sciencist’s toolbox). We hope this module can provide you with the necessary knowledge in becoming a social and geographic data scientist in the future. Weekly topics Week Date Topic Moodle GitHub 1 11/01/2021 Introduction to SocGeoDataScience x 2 18/01/2021 Statistical Inference and Causality x 3 25/01/2021 GeoAI: Introduction to Deep Learning x 4 01/02/2021 Keynote: Spatial Data Science (Carto) x 5 08/02/2021 GeoAI: Convolutional Neural Networks x reading week 6 22/02/2021 GeoAI: Spatial Data Science Applications x 7 01/03/2021 GeoAI: Spatial-Temporal Mobility Analysis x 8 08/03/2021 Data Sciencist’s Toolbox: Unix tools x 9 15/03/2021 Data Sciencist’s Toolbox: Web Visualisation x 10 22/03/2021 A look into the future of SocGeoDataScience x Moodle All important information and communication in relation to this module will be provided on Moodle. Even though some of the short lecture videos and practical materials will be hosted on this webpage, do check on Moodle regularly to see if there are any updates or important messages. Useful additional resources Besides the mandatory and recommended reading for this course, there are some additional resources that are worth checking out: MIT’s introduction course on mastering the command line: The Missing Semester of Your CS Education A useful tool to unpack command line instructions: explainshell.com Online resource to develop and check your regular expressions: regexr.com Selecting colour palettes for your map making and data visualisation: colorbrewer 2.0 Office hours Office hours with Dr Justin van Dijk (Tuesdays between 15h00 and 16h00) can be booked here. Department of Geography, https://www.mappingdutchman.com/↩︎ Department of Geography, https://www.geog.ucl.ac.uk/people/academic-staff/stephen-law↩︎ "],
["introduction.html", "1 Introduction", " 1 Introduction The first week of Advanced Topics in Social and Geographic Data Science will introduce the module, why is it useful and what you will learn. You will be given a recap to the introduction to social and geographic data science specifically on the machine learning workflow. We will then give instructions to the reading group and the coursework assessment. This week’s practical component will focus on installing a new virtual environment for the module, accessing Jupyter Notebook locally and a recap on a typical machine learning workflow in Python. We will also get everyone familiar with Google Colab Web service. This week plays a formative role in providing you with the baseline knowledge for the module. This week’s material is available on Moodle. "],
["statistical-inference-and-causality.html", "2 Statistical Inference and Causality 2.1 Introduction 2.2 Statistical inference 2.3 Causality 2.4 Take home message 2.5 Attributions", " 2 Statistical Inference and Causality 2.1 Introduction Statistical inference is the process of using data analysis to deduce properties of an underlying distribution of probability. In other words: statistical inference is the procedure through which we try to make inferences about a population based on characteristics of this population that have been captured in a sample. This includes uncovering the association between variables as well as establishing causal relationships. However, correlation does not imply causation and identifying causal relationships from observational data is not trivial. “Correlation does not imply causation” (Any statistician). This week we will dive a little deeper into statistical inference and how to establish causal relationships by taking a small dive into the field of study of econometrics. This week’s lecture video is provided by an absolute expert in the field of econometrics: Dr Idahosa from the University of Johannesburg. This week is further structured by reading material, a tutorial in R with a ‘hands-on’ application of the techniques covered in the lecture video, and a seminar on Tuesday. Let’s get to it! Video: Introduction W01 [Lecture slides] [Watch on MS stream] 2.1.1 Reading list Please find the reading list for this week below. Core reading Bzdok, D., Altman, and M. Krzywinski. 2018. Statistics versus machine learning. Nature Methods 15: 233-234. [Link] Idahosa, L. O., Marwa, N., and J. Akotey. 2017. Energy (electricity) consumption in South African hotels: A panel data analysis. Energy and Buildings 156: 207-217. [Link] Stock, J. and M. Watson. 2019. Chapter 1: Economic Questions and Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.43-54. Harlow: Pearson Education Ltd. [Link] Stock, J. and M. Watson. 2019. Chapter 10: Regression with Panel Data. In: Stock, J. and M. Watson. Introduction to Econometrics, pp.362-382. Harlow: Pearson Education Ltd. [Link] Supplementary reading Idahosa, L. O. and J. T. van Dijk. 2016. South Africa: Freedom for whom? Inequality, unemployment and the elderly. Development 58(1): 96-102. [Link] 2.1.2 Technical Help session Every Thursday between 13h00-14h00 you can join the Technical Help session on Microsoft Teams. The session will be hosted by Alfie. He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you are having in the Technical Help channel on the GEOG0125 Team so that Alfie can help to find a solution. 2.2 Statistical inference Where during the remainder of this module we will predominantly focus on different machine learning methods and techniques and the data scientist’s toolbox, this week we will focus on statistical inference. Although there is considerable overlap between machine learning and statistics, sometimes even involving the same methods, the major difference between machine learning and statistics is their purpose. Where machine learning models are designed to make accurate predictions, statistical models are designed for inference about the relationships between variables. “Statistics draws population inferences from a sample, and machine learning finds generalizable predictive patterns” (Bzdok et al. 2018). One way to analyse the relationships between variables is by a linear regression. Linear regression models offer a very interpretable way to model the association between variables. A linear regression model is used to find the line that minimises the mean squared error across all data to establish the relationship between a response variable one or more explanatory (independent) variables. The case of one explanatory variable is called a simple linear regression; if more explanatory variables are used it is called multiple regression. The purpose of a regression model is to predict a target variable \\(Y\\) according to some other variable \\(X\\) or variables \\(X_{1}\\), \\(X_{2}\\), etc. 2.3 Causality 2.3.1 What is causality? Regression cares about correlation: what happens to \\(Y\\) when \\(X=x\\). However, correlation does not imply causation. A great example can be found in a research project reporting on the relationships between chocolate consumption and the probability of winning a Nobel price. The results suggest that countries in which the population consumes on average a large amount of chocolate per annum, spawn more Nobel laureates per capita than countries in which the population on average consume less chocolate. There is a clear correlation in the data, but there should not exist an actual causal relationship between these two variables (Well, some research suggests it still remains unclear whether the correlation is spurious or whether it is an indication for hidden variables …) The golden standard to establish a causal relationship is to set-up and execute a randomised control trial. Think of the many large-scale randomised control trials that are currently taking place to test the safety and effectiveness of various candidate coronavirus vaccines. However, it is not always possible to set up a randomised control trial. Sometimes this has to do with the nature of the relationship being investigated (e.g. establishing the effects of policy changes), but there could also be financial and ethical constraints. As an alternative, one could try to identify causal relationship from observational data. This is known as causal inference and most research in econometrics is concerned with retrieving valid estimates using different regression methods. Note The distinction between causal and non-causal relationships is crucial and heavily depends on your research questions. If you are trying to classify Google Street view images and predict whether the photo contains an office building or a residential building, you want to create a model that predicts this as good as possible. However, you do not care about what caused the building in the photo to be an office building or a residential building. That being said: almost any question is causal and where statistics is used in almost any field of inquiry, few pay proper attention to understanding causality. By now you may wonder, sure, but what then is causality? We can say that \\(X\\) causes \\(Y\\) if we were to change the value of \\(X\\) without changing anything else then as a result \\(Y\\) would also change. A simple example: if you switch on the light switch, your light bulb will go on. The action of flipping the light switch causes your light to go on. This being said: it does not mean that \\(X\\) is necessarily the only thing that causes \\(Y\\) (e.g. the light bulb is burnt out or the light was already on) and perhaps a better way of phrasing it is to say that there is a causal relationship between variables if \\(X\\) changes the probability of \\(Y\\) happening or changing. 2.3.2 The problem with causality The problem with establishing a causal relationship is that in many cases you cannot ‘switch on’ or ‘switch off’ a characteristic. Let’s go through this by thinking whether some \\(X\\) causes \\(Y\\). Our \\(X\\) is coded as 0 or 1, for instance, 0 if a person has not received a coronavirus vaccination and 1 if a person has received a coronavirus vaccination. \\(Y\\) is some numeric value. So how do we check if \\(X\\) causes \\(Y\\)? What we would need to do for everyone in our sample is to check what happens to \\(Y\\) when we make \\(X=0\\) and what happens to \\(Y\\) when we make \\(X=1\\). Obviously, this is a problem! You cannot both have \\(X=0\\) and \\(X=1\\): you either got inoculated against the coronavirus or you did not. This means that if \\(X=1\\) you can measure what the value of \\(Y\\) is, but you do not know what the value of \\(Y\\) would have been if \\(X=0\\). The solution you may come up with is to compare \\(Y\\) between individuals who have \\(X=0\\) and \\(X=1\\). However, there is another problem: there could be all kinds of reasons on why \\(Y\\) differs between individuals that are not necessarily related to \\(X\\). Note This section heavily borrows material and explanations from Nick Huntington-Klein’s excellent ECON 305: Economics, Causality, and Analytics module, do have a look if you want to learn more about this topic. This brings us to econometrics and causal inference: the main goal of causal inference is to make the best possible estimation of what \\(Y\\) would have been if \\(X\\) would have been different, the so-called counterfactual. As we cannot always use an experiment in which we can randomly assign \\(X\\) so that we know that on average people with \\(X=1\\) and the same as people with \\(X=2\\), we have to come up with a model to figure out what the counterfactual would do. In the following, we will explore two ways of doing this through so-called fixed effects models and random effects models in the situation in which we have data points for each observation across time (i.e. longitudinal data or panel data). Video: Panel Data Analysis [Lecture slides] [Watch on MS stream] 2.3.3 Fixed effects models Fixed effects are variables that are constant across individuals; these variables, like age, sex, or ethnicity, typically do not change over time or change over time at a constant rate. As such, they have fixed effects on a dependent variable \\(Y\\). As such, using a fixed effects model you can explore the relationship between variables within an entity (which could be persons, companies, countries, etc.). Each entity has its own individual characteristics that may or may not influence the dependent variable. When using a fixed effects model, we assume that something within the entity may impact or bias the dependent variables and we need to control for this. A fixed effects model does this by removing the characteristics that do not change over time so that we can assess the net effect of the independent variables on the dependent variable. Let’s try to to apply a fixed effects model in R. For this we will use a data set containing some panel data. The data set contains fictional data, for different countries and years, for an undefined variable \\(Y\\) that we want to explain with some other variables. File download File Type Link Example Panel Data csv Download # load libraries library(tidyverse) library(plm) library(car) library(gplots) library(tseries) library(lmtest) # read data country_data &lt;- read_csv(&#39;raw/w02/paneldata.csv&#39;) # inspect head(country_data) ## # A tibble: 6 x 6 ## country year y x1 x2 x3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 1990 1342787840 0.278 -1.11 0.283 ## 2 A 1991 -1899660544 0.321 -0.949 0.493 ## 3 A 1992 -11234363 0.363 -0.789 0.703 ## 4 A 1993 2645775360 0.246 -0.886 -0.0944 ## 5 A 1994 3008334848 0.425 -0.730 0.946 ## 6 A 1995 3229574144 0.477 -0.723 1.03 Upon inspecting the dataframe you can see that the data contains 8 different fictional countries. For each country we have several years of data: three independent variables names \\(X_{1}\\), \\(X_{2}\\), and \\(X_{3}\\) and a dependent variable \\(y\\). As this is a panel dataset we have to declare it as such using the plm.data() function from the plm package. The plm package is a library dedicated to panel data analysis. # create a panel data object country_panel &lt;- pdata.frame(country_data, index=c(&#39;country&#39;,&#39;year&#39;)) # inspect country_panel ## country year y x1 x2 x3 ## A-1990 A 1990 1342787840 0.2779037 -1.1079559 0.28255358 ## A-1991 A 1991 -1899660544 0.3206847 -0.9487200 0.49253848 ## A-1992 A 1992 -11234363 0.3634657 -0.7894840 0.70252335 ## A-1993 A 1993 2645775360 0.2461440 -0.8855330 -0.09439092 ## A-1994 A 1994 3008334848 0.4246230 -0.7297683 0.94613063 ## [ reached &#39;max&#39; / getOption(&quot;max.print&quot;) -- omitted 65 rows ] Although the data looks the same, you can see that the row index has been updated to reflect the country and year variables. Let’s inspect the data using a boxplot as well as a conditioning plot. A coplot is a method for visualising interactions in your data set: it shows you how some variables are conditional on some other set of variables. So, for our panel data set, we can look at the variation of \\(Y\\) over time by country. The bars at top indicate the countries position from left to right starting on the bottom row. # create a quick box plot scatterplot(y ~ year, data=country_panel) ## [1] &quot;11&quot; &quot;54&quot; &quot;45&quot; &quot;55&quot; &quot;46&quot; &quot;36&quot; &quot;59&quot; # create a quick conditioning plot coplot(y ~ year|country, data=country_panel, type=&#39;b&#39;) The graphs show that there \\(Y\\) is variable both over time and between countries. Let’s also have a look at the heterogeneity of our data by plotting the means, and the 95% confidence interval around the means, of \\(Y\\) across time and across countries. # means across years plotmeans(y ~ year, data=country_panel) # means across countries plotmeans(y ~ country, data=country_panel) There is clearly some heterogeneity across the countries and across years. However, the basic Ordinary Least Squares (OLS) regression model does not consider this heterogeneity: # run an OLS ols &lt;- lm(y ~ x1, data=country_panel) # summary summary(ols) ## ## Call: ## lm(formula = y ~ x1, data = country_panel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.546e+09 -1.578e+09 1.554e+08 1.422e+09 7.183e+09 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.524e+09 6.211e+08 2.454 0.0167 * ## x1 4.950e+08 7.789e+08 0.636 0.5272 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.028e+09 on 68 degrees of freedom ## Multiple R-squared: 0.005905,\tAdjusted R-squared: -0.008714 ## F-statistic: 0.4039 on 1 and 68 DF, p-value: 0.5272 The results show that there is no significant statistical relationship between \\(X_{1}\\) and the dependent variable \\(Y\\) as the model tries to explain all variability in the data at once. You can clearly see this in the plot below: # plot the results scatterplot(country_panel$y ~ country_panel$x1, boxplots=FALSE, smooth=FALSE, pch=19, col=&#39;black&#39;) # add the OLS regression line abline(lm(y ~ x1, data=country_panel),lwd=3, col=&#39;red&#39;) One way of getting around this, and fixing the effects of the country variable, is by using a model incorporating dummy variables through a Least Squares Dummy Variable model (LSDV). # run a LSDV fixed_dum &lt;- lm(y ~ x1 + factor(country) - 1, data=country_panel) # summary summary(fixed_dum) ## ## Call: ## lm(formula = y ~ x1 + factor(country) - 1, data = country_panel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -8.634e+09 -9.697e+08 5.405e+08 1.386e+09 5.612e+09 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## x1 2.476e+09 1.107e+09 2.237 0.02889 * ## factor(country)A 8.805e+08 9.618e+08 0.916 0.36347 ## factor(country)B -1.058e+09 1.051e+09 -1.006 0.31811 ## factor(country)C -1.723e+09 1.632e+09 -1.056 0.29508 ## factor(country)D 3.163e+09 9.095e+08 3.478 0.00093 *** ## factor(country)E -6.026e+08 1.064e+09 -0.566 0.57329 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.796e+09 on 62 degrees of freedom ## Multiple R-squared: 0.4402,\tAdjusted R-squared: 0.368 ## F-statistic: 6.095 on 8 and 62 DF, p-value: 8.892e-06 The least square dummy variable model (LSDV) provides a good way to understand fixed effects. By adding the dummy for each country, we are now estimating the pure effect of \\(X_{1}\\) on \\(Y\\). Each dummy is absorbing the effects that are particular to each country. Where the independent variable \\(X_{1}\\) was not significant in the OLS model, after controlling for differences across countries, \\(X_{1}\\) became significant in the LSDV model. You can clearly see why this is happening in the plot below: # plot the results scatterplot(fixed_dum$fitted ~ country_panel$x1|country_panel$country, boxplots=FALSE, smooth=FALSE, col=&#39;black&#39;) ## Warning in scatterplot.default(X[, 2], X[, 1], groups = X[, 3], xlab = xlab, : number of groups exceeds number of available colors ## colors are recycled # add the LSDV regression lines abline(lm(y ~ x1, data=country_panel),lwd=3, col=&#39;red&#39;) We can also run a country-specific fixed effects model by using specific intercepts for each country. We can achieve this by using the plm package. # run a fixed effects model fixed_effects &lt;- plm(y ~ x1, data=country_panel, model=&#39;within&#39;) # summary summary(fixed_effects) ## Oneway (individual) effect Within Model ## ## Call: ## plm(formula = y ~ x1, data = country_panel, model = &quot;within&quot;) ## ## Balanced Panel: n = 7, T = 10, N = 70 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -8.63e+09 -9.70e+08 5.40e+08 0.00e+00 1.39e+09 5.61e+09 ## ## Coefficients: ## Estimate Std. Error t-value Pr(&gt;|t|) ## x1 2475617742 1106675596 2.237 0.02889 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Total Sum of Squares: 5.2364e+20 ## Residual Sum of Squares: 4.8454e+20 ## R-Squared: 0.074684 ## Adj. R-Squared: -0.029788 ## F-statistic: 5.00411 on 1 and 62 DF, p-value: 0.028892 Here you can check the individual intercepts through fixef(fixed). The coefficient of \\(X_{1}\\) indicates how much \\(Y\\) changes on average over time per country: as you can see the results are identical to the results we got from running the LSDV model. Arguably, however, running your model with explicit dummy variables is more informative. 2.3.4 Random effects models Random effects are the opposite of fixed effects. Contrary to fixed effects, random effects are random and difficult to predict. As such, the effect they will have on a dependent variable \\(Y\\) is not constant. Think of the cost of renting a one bedroom appartement: rental prices vary greatly depending on location. If you have reason to believe that differences across entities have some influence on the dependent variable that is not time-dependent, then you would use a random effects model approach over a fixed effects model approach. # run a random effects model random_effects &lt;- plm(y ~ x1, data=country_panel, model=&#39;random&#39;) # summary summary(random_effects) ## Oneway (individual) effect Random Effect Model ## (Swamy-Arora&#39;s transformation) ## ## Call: ## plm(formula = y ~ x1, data = country_panel, model = &quot;random&quot;) ## ## Balanced Panel: n = 7, T = 10, N = 70 ## ## Effects: ## var std.dev share ## idiosyncratic 7.815e+18 2.796e+09 0.873 ## individual 1.133e+18 1.065e+09 0.127 ## theta: 0.3611 ## ## Residuals: ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -8.94e+09 -1.51e+09 2.82e+08 0.00e+00 1.56e+09 6.63e+09 ## ## Coefficients: ## Estimate Std. Error z-value Pr(&gt;|z|) ## (Intercept) 1037014329 790626206 1.3116 0.1896 ## x1 1247001710 902145599 1.3823 0.1669 ## ## Total Sum of Squares: 5.6595e+20 ## Residual Sum of Squares: 5.5048e+20 ## R-Squared: 0.02733 ## Adj. R-Squared: 0.013026 ## Chisq: 1.91065 on 1 DF, p-value: 0.16689 Interpretation of the coefficients is a little bit tricky since they include both the within-entity and between-entity effects. In this case, the data represents the average effect of \\(X_{1}\\) over \\(Y\\) when \\(X\\) changes across time and between countries by one unit. If you are not sure whether you should run a fixed effects or a random effects model, you can run a Hausman test to help you with the decision. The Hausman test tests whether the variation across entities (i.e. countries in our example) is uncorrelated with the independent variables. The null hypothesis is that there is no such correlation: if the Hausman specification test comes back significant then it means that you should use a fixed effects model. You run it by comparing the results from both models! phtest(fixed_effects, random_effects) ## ## Hausman Test ## ## data: y ~ x1 ## chisq = 3.674, df = 1, p-value = 0.05527 ## alternative hypothesis: one model is inconsistent According to the Hausman test, we should use the random effects model to estimate the relationship between \\(x_1\\) and our \\(Y\\). Note Normally, this is not the end of it as there is a sequence of tests that can and should be performed to make sure the model is valid. Testing for hetersokedaticity and stochastic trends, for instance. However, this is out of scope of the current module. 2.4 Take home message Different models are used for different research questions: the question guides the model. What is important to keep in mind, especially when working with machine learning models, is that not all models are suitable to say something about the relationships between variables. Let alone on whether variable \\(X\\) is the cause of change in variable \\(Y\\). In this week’s material, we really wanted to introduce you to some models that do explicitly look at relationships between variables from an econometrics point of view - a very tiny sneak peak if you may. Of course, there are many other advanced econometric models such as models using instrumental variables and difference-in-differences models. Inference is difficult but important in social science, and some of these traditional statistical/econometric method try to get more reliable estimates. What we will be learning in the next couple of weeks will be less interpretable/explanable but it is important to keep some of the issues that came to light during this week’s material in mind. That is it for this week! 2.5 Attributions This week’s content and practical uses content and inspiration from: Torres-Reyna, Oscar. 2010. Getting Started in Fixed/Random Effects Models using R. [Link] Huntington-Klein, Nick. 2019. ECON 305: Economics, Causality, and Analytics. Lecture 13: Causality. [Link] "],
["introduction-to-deep-learning.html", "3 Introduction to Deep Learning", " 3 Introduction to Deep Learning This week, we will introduce you to the topic of Deep Learning, its history and its uses in digital humanities, robotics, climate science, geography and social science. We will discuss the principles and algorithms of training and evaluating a deep neural network as an extension of a linear regression model. We will also present the strength and weakness of such techniques. The practical component of the week will allow you to practice and deep learning models, which are highly transferable skills in the future. This week’s material is available on Moodle. "],
["keynote-spatial-data-science.html", "4 Keynote: Spatial Data Science", " 4 Keynote: Spatial Data Science This week we will have a guest lecture from Miguel Alvarez Garcia @Carto on CartoFrame and Spatial Data Science. Miguel is a Data Scientist at CARTO and has experience in optimisation and machine learning. His current work focuses on solving problems with a strong geospatial component in various sectors such as logistics, utilities, retail, and environmental. Details on this guest lecture are available on Moodle. "],
["convolutional-neural-networks.html", "5 Convolutional Neural Networks", " 5 Convolutional Neural Networks This week, we will introduce you to the topic of Convolutional Neural Network, a special case of Neural Network. In particular, we will describe the various CNN architecture and the useful technique of transfer learning. We will discuss the principles behind the different convolutional layers and activation functions. We will also give a case study on street frontage classification and object detection. The practical component of the week will allow you to practice running convolutional neural network models. This week’s material is available on Moodle. "],
["spatial-data-science-applications.html", "6 Spatial Data Science Applications", " 6 Spatial Data Science Applications This week, we will introduce you the topic of applying deep neural networks for spatial data science applications. In particular we will describe the use of multi-modality data in machine learning. In particular we will describe the use of multi-modal data for socio-economic predictions for two different papers. This week’s material is available on Moodle. "],
["spatial-temporal-mobility-analysis.html", "7 Spatial-Temporal Mobility Analysis 7.1 Introduction 7.2 GPS data 7.3 GPS data classification 7.4 Take home message 7.5 Attributions", " 7 Spatial-Temporal Mobility Analysis 7.1 Introduction Against the background of unprecedented growth in private vehicle ownership and the entrenchment of the private car in everyday life, the past decades have seen a growing and ongoing academic and policy debate on how to encourage individuals to change to more sustainable ways of travelling. More recently, researchers have started to build on so-called location-aware technologies, exploring innovative methods to more accurately capture, visualise, and analyse individual spatiotemporal travel patterns: information that can be used to formulate strategies to accommodate the increasing demand for transport vis-à-vis growing environmental and societal concerns. This week we will be looking at capturing mobility data with Global Positioning Systems. We will further use two types of Machine Learning classifiers, specifically Support Vector Machines and tree-based methods, to classify labeled GPS data into stay and move points. This week is structured around three short videos as well as a tutorial in R with a ‘hands-on’ application of GPS data classification. Let’s get to it! Video: Introduction W07 [Lecture slides] [Watch on MS stream] 7.1.1 Reading list Please find the reading list for this week below. Core reading Broach, J. et al.. 2019. Travel mode imputation using GPS and accelerometer data from a multi-day travel survey. Journal of Transport Geography 78: 194-204. [Link] Bohte, W. and K. Maat. 2009. Deriving and validating trip purposes and travel modes for multi-day GPS-based travel surveys: A large-scale application in the Netherlands. Transportation Research Part C: Emerging Technologies 17(3): 285–297. [Link] Feng, T and H. Timmermans. 2016. Comparison of advanced imputation algorithms for detection of transportation mode and activity episode using GPS data. Transportation Planning and Technology 39(2): 180–194. [Link] Nitsche, P. et al.. 2014. Supporting large-scale travel surveys with smartphones - a practical approach. Transportation Research Part C: Emerging Technologies 43: 212–221. [Link] Supplementary reading Behrens, R. et al.. 2006. Collection of passenger travel data in Sub-Saharan African cities: Towards improving survey instruments and procedures. Transport Policy 13: 85-96. [Link] Behrens, R. and Del Mistro, R. 2010. Shocking habits: Methodological issues in analyzing changing personal travel behavior over time. International Journal of Sustainable Transportation 4(5): 253-271. [Link] Van de Coevering, P. et al.. 2021. Causes and effects between attitudes, the built environment and car kilometres: A longitudinal analysis. Journal of Transport Geography 91: 102982. [Link] Van Dijk, J. 2018. Identifying activity-travel points from GPS-data with multiple moving windows. Computers, Environment and Urban Systems 70: 84-101. [Link] Wolf, J. 2000. Using GPS data loggers to replace travel diaries in the collection of travel data. Doctoral dissertation. Atlanta, Georgia: Georgia Institute of Technology. [Link] 7.1.2 Technical Help session Every Thursday between 13h00-14h00 you can join the Technical Help session on Microsoft Teams. The session will be hosted by Alfie. He will be there for the whole hour to answer any question you have live in the meeting or any questions you have formulated beforehand. If you cannot make the meeting, feel free to post the issue you are having in the Technical Help channel on the GEOG0125 Team so that Alfie can help to find a solution. 7.2 GPS data Mobility is a central aspect of everyday life. In its simplest form, human mobility refers to the movement of individuals from location A to location B. This can be a relocation from one city to another city, as well as a trip from home to work. Transport systems provide the physical nodes and linkages that facilitate this mobility. However, transport systems and road networks in many cities around the world are under pressure as a result of unparalleled growth in private vehicle ownership and increasingly complex and fragmented travel patterns. Particularly in urban areas, this is problematic because it leads to problems such as congestion, accidents, road decay, and reduced accessibility. As such, governments and researchers throughout the world have started to recognise the need to curtail demand for private road transport. “Technological breakthroughs [alone] are not going to provide the silver bullet for the mitigation of climate change and energy security threats caused by the transport sector” (Stradling and Anable, 2008: 195) The realisation that increasing road infrastructure and improvements in car technology are not sufficient to address the transport problems around the world has led to the idea that transport planning should shift from supply-side to demand-side passenger transport planning. For this, accurate data are required on individual spatio-temporal behaviour. Travel data collection methods can roughly be classified into two, not per semutually exclusive, methods. The first method uses self-reported data, such as data collected through telephone-assisted interviews, computer-assisted interviews, and pen-and-paper interviews. The second method relies on passively collected data, such as data collected through call-detail records and GPS data. Technological developments in the field of location-aware technologies, GPS in particular, have greatly enhanced opportunities to collect accurate data on human spatiotemporal behaviour. GPS data need to be collected and analysed systematically to be intelligible for transport researchers and policy makers. Moreover, the challenges inherent to mobile data collection techniques include not only harnessing the tools to obtain geo-referenced data, but also the development of new skills sets for cleaning, analysing, and interpreting these data. Video: GPS data for mobility research [Lecture slides] [Watch on MS stream] 7.3 GPS data classification Where GPS technology can precisely register the spatiotemporal elements of activity-travel behaviour, travel characteristics need to be imputed from the data. As such, throughout the last decade or so, a plethora of methods has been developed for identifying trips, activities, and travel modes from raw GPS trajectories. These methods range from deterministic (rule-based) methods to advanced machine learning algorithms. Here, we will focus on using two types of machine learning techniques (Support Vector Machines and tree-based methods) to classify labelled GPS points into stay and move points. Video: GPS data classification with ML techniques [Lecture slides] [Watch on MS stream] The segmentation of GPS data into activity and travel episodes is often the first step in a more elaborate process of identifying activity types and transport modes. A major issue with GPS data imputation, however, is the necessity of a ground truth to test whether the imputation algorithm correctly categorises GPS points into activity (stay) points and trips (move) points. We will be using a set of 50 artificially created GPS trajectories that all contain a sequence of stays and moves in Cape Town, South Africa. For the data that we will use, the artificial GPS ‘records’ a measurement every 60 seconds. To further simulate noise in the data, a random sample comprising of 50 per cent of the data points was taken. Besides these raw GPS data, we also have access to a basic road network lay out of the Mother City. File download File Type Link Cape Town GPS and road data shp Download We will start by downloading the files, importing both into R, and looking at what we will be working with. Be careful not to plot the road network file. Because the road network contains around 120,000 individual road segments it will take a long time to draw. Rather have a look at the data in QGIS, which is much more capable of on the fly displaying a large number of features. # load libraries library(tidyverse) library(sf) library(tmap) # read gps data gps &lt;- read_sf(&#39;raw/w07/gps_cape_town.shp&#39;) # read road data road &lt;- read_sf(&#39;raw/w07/roads_cape_town.shp&#39;) # inspect gps ## Simple feature collection with 27019 features and 9 fields ## geometry type: POINT ## dimension: XY ## bbox: xmin: 18.34291 ymin: -34.19419 xmax: 19.06105 ymax: -33.66433 ## CRS: 4148 ## # A tibble: 27,019 x 10 ## stop type duration point_id_n timestamp mode track_id move activity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 1 more variable: geometry &lt;POINT [°]&gt; # inspect road ## Simple feature collection with 125024 features and 1 field ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: 18.34051 ymin: -34.3001 xmax: 19.12007 ymax: -33.62755 ## CRS: 4148 ## # A tibble: 125,024 x 2 ## id geometry ## &lt;dbl&gt; &lt;LINESTRING [°]&gt; ## 1 1 (18.93353 -34.16301, 18.93345 -34.1631, 18.93335 -34.16318, 18.93… ## 2 2 (18.45615 -34.27366, 18.4545 -34.27321, 18.45298 -34.2728, 18.452… ## 3 3 (18.45615 -34.27366, 18.45674 -34.27201, 18.45683 -34.27181, 18.4… ## 4 4 (18.85612 -34.25045, 18.85621 -34.2506, 18.85633 -34.25087, 18.85… ## 5 5 (19.12007 -34.25944, 19.11977 -34.25952, 19.11936 -34.25961, 19.1… ## 6 6 (18.50105 -33.98948, 18.50102 -33.98942, 18.50105 -33.98935, 18.5… ## 7 7 (18.65381 -34.00991, 18.65355 -34.00979, 18.65251 -34.00931, 18.6… ## 8 8 (18.63738 -34.07209, 18.63846 -34.07193, 18.63924 -34.07183, 18.6… ## 9 9 (18.44389 -34.03779, 18.44389 -34.03798, 18.44388 -34.03826, 18.4… ## 10 10 (18.64958 -34.04498, 18.64729 -34.04547, 18.64228 -34.04652, 18.6… ## # … with 125,014 more rows # inspect names(gps) ## [1] &quot;stop&quot; &quot;type&quot; &quot;duration&quot; &quot;point_id_n&quot; &quot;timestamp&quot; ## [6] &quot;mode&quot; &quot;track_id&quot; &quot;move&quot; &quot;activity&quot; &quot;geometry&quot; # project into Hartbeesthoek94 gps &lt;- gps %&gt;% st_transform(crs=&#39;epsg:2053&#39;) road &lt;- road %&gt;% st_transform(crs=&#39;epsg:2053&#39;) # plot gps points tm_shape(gps) + tm_dots() The road network file is relatively simple and only contains road network segments - no additional information (e.g. maximum speed, road type, etc.) is available. The GPS data themselves contain several fields: Column heading Description stop Stop number within GPS trajectory type Type of stop (i.e. short, medium, long) duration Duration of stop in seconds point_id_n Unique point identifier within GPS trajectory timestamp Time the point was recorded mode Travel mode track_id Unique ID for GPS trajectory move Whether point is a move or a stay activity Whether point is move (inlcuding mode) or stay 7.3.1 GPS data preparation The move variable is the variable we will try to predict. Before we split our data into a train and test set, however, we will need to derive some features from our raw GPS data that we can use as our input features: speed, point density, and distance to nearest road segment. As we want to derive information of consecutive measurements, the order of the data is very important right now as the points form a trajectory: so before we calculate anything we start by making sure that the data are ordered correctly: by track_id and timestamp. # order data so that we do not mix separate trajectories gps &lt;- gps %&gt;% arrange(track_id,timestamp) # calculate distance between consecutive points // could take a few minutes gps &lt;- gps %&gt;% group_by(track_id) %&gt;% mutate( lead_geom = geometry[row_number() + 1], dist = st_distance(lead_geom,geometry, by_element=TRUE, which=&#39;Euclidean&#39;) ) %&gt;% ungroup() # inspect gps ## Simple feature collection with 27019 features and 10 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 12 ## stop type duration point_id_n timestamp mode track_id move activity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 3 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m] # calculate time between consecutive points gps &lt;- gps %&gt;% group_by(track_id) %&gt;% mutate( lead_time = strptime(timestamp[row_number() + 1], format=&#39;%d/%m/%Y %H:%M:%S&#39;), diff = difftime(lead_time,strptime(timestamp, format=&#39;%d/%m/%Y %H:%M:%S&#39;), units=&#39;secs&#39;) ) %&gt;% ungroup() # inspect gps ## Simple feature collection with 27019 features and 12 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 14 ## stop type duration point_id_n timestamp mode track_id move activity ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 5 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt; # calculate speed in kilometres per hour gps &lt;- gps %&gt;% mutate(speed = as.integer(dist)/as.integer(diff)*3.6) # inspect gps ## Simple feature collection with 27019 features and 13 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 15 ## stop type duration point_id_n timestamp mode track_id move activity ## * &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 6 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt;, ## # speed &lt;dbl&gt; Because we are using consecutive rows to calculate our speed, all of the last measurements within each of the 50 trajectories will not be assigned a value. However, because we do not really want to throw these points out, we will simply assign these points the same speed value as the last point that did get a value assigned. # fill missing speeds gps &lt;- gps %&gt;% fill(speed, .direction=&#39;down&#39;) Another useful input feature would be a local point density: for each point it would be useful to know how many other points are in its vicinity, for instance, because a clustering of points could be indicative of an activity. Because vicinity is difficult to define, we will use three distance thresholds to create these local point densities: 100m, 250m, and 500m. # distance thresholds gps_100m &lt;- st_buffer(gps,100) gps_250m &lt;- st_buffer(gps,250) gps_500m &lt;- st_buffer(gps,500) # loop through trajectories and count the number of points falling within the three distance thresholds # // this could take a few minutes df &lt;- gps[0,] for (t in seq(1,50)) { # filter gps trajectory gps_sel &lt;- filter(gps, as.integer(track_id)==t) # filter buffer gps_100m_sel &lt;- filter(gps_100m, as.integer(track_id)==t) gps_250m_sel &lt;- filter(gps_250m, as.integer(track_id)==t) gps_500m_sel &lt;- filter(gps_500m, as.integer(track_id)==t) # intersect gps_sel$buf100 &lt;- lengths(st_intersects(gps_sel, gps_100m_sel)) gps_sel$buf250 &lt;- lengths(st_intersects(gps_sel, gps_250m_sel)) gps_sel$buf500 &lt;- lengths(st_intersects(gps_sel, gps_500m_sel)) # bind results df &lt;- rbind(df,gps_sel) } # rename gps &lt;- df # inspect gps ## Simple feature collection with 27019 features and 16 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 18 ## stop type duration point_id_n timestamp mode track_id move activity ## * &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 9 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt;, ## # speed &lt;dbl&gt;, buf100 &lt;int&gt;, buf250 &lt;int&gt;, buf500 &lt;int&gt; Great. We now have a speed variable as well as three local density variables. The last thing we will need to do is for each point calculate the distance to the nearest road segment. # for each point: get nearest road feature # // this could take a few minutes nearest_road &lt;- st_nearest_feature(gps, road) # for each point: get the distance to the nearest road feature # // this could take a few minutes nearest_road_dst &lt;- st_distance(gps, road[nearest_road,], by_element=TRUE, which=&#39;Euclidean&#39;) # assign values to gps data set gps$road_dist &lt;- nearest_road_dst # inspect gps ## Simple feature collection with 27019 features and 17 fields ## Active geometry column: geometry ## geometry type: POINT ## dimension: XY ## bbox: xmin: 918250.7 ymin: 3775604 xmax: 986130.2 ymax: 3836086 ## CRS: epsg:2053 ## # A tibble: 27,019 x 19 ## stop type duration point_id_n timestamp mode track_id move activity ## * &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 MEDI… 30 1 04/02/19… &lt;NA&gt; 001 STAY STAY ## 2 1 MEDI… 30 2 04/02/19… &lt;NA&gt; 001 STAY STAY ## 3 1 MEDI… 30 5 04/02/19… &lt;NA&gt; 001 STAY STAY ## 4 1 MEDI… 30 9 04/02/19… &lt;NA&gt; 001 STAY STAY ## 5 1 MEDI… 30 10 04/02/19… &lt;NA&gt; 001 STAY STAY ## 6 1 MEDI… 30 12 04/02/19… &lt;NA&gt; 001 STAY STAY ## 7 1 MEDI… 30 14 04/02/19… &lt;NA&gt; 001 STAY STAY ## 8 1 MEDI… 30 16 04/02/19… &lt;NA&gt; 001 STAY STAY ## 9 1 MEDI… 30 18 04/02/19… &lt;NA&gt; 001 STAY STAY ## 10 1 MEDI… 30 22 04/02/19… &lt;NA&gt; 001 STAY STAY ## # … with 27,009 more rows, and 10 more variables: geometry &lt;POINT [m]&gt;, ## # lead_geom &lt;POINT [m]&gt;, dist [m], lead_time &lt;dttm&gt;, diff &lt;drtn&gt;, ## # speed &lt;dbl&gt;, buf100 &lt;int&gt;, buf250 &lt;int&gt;, buf500 &lt;int&gt;, road_dist [m] 7.3.2 GPS data classification Now we have added some useful variables to our raw GPS trajectories, we can scale them and move on to our classification algorithms. We will use a C5.0 boosted decision tree, a random forest, and a support vector machine. We will use a test/train split of 70/30. # libraries library(e1071) library(randomForest) library(C50) library(caret) # assign point to train and test gps &lt;- gps %&gt;% mutate(train = if_else(runif(nrow(gps)) &lt; 0.7, 1, 0)) # create train set, select variables, confirm data types where necessary gps_train &lt;- gps %&gt;% filter(train==1) %&gt;% select(speed,buf100,buf250,buf500,road_dist,move) %&gt;% mutate(road_dist=as.numeric(road_dist)) # drop geometry gps_train &lt;- st_drop_geometry(gps_train) # scale gps_train[1:5] &lt;- lapply(gps_train[1:5], function(x) scale(x)) # inspect gps_train ## # A tibble: 18,951 x 6 ## speed[,1] buf100[,1] buf250[,1] buf500[,1] road_dist[,1] move ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -0.738 -0.479 -0.760 -0.903 -0.745 STAY ## 2 -0.519 -0.457 -0.760 -0.903 -0.246 STAY ## 3 -0.625 -0.413 -0.760 -0.903 -0.277 STAY ## 4 -0.563 -0.545 -0.760 -0.903 1.73 STAY ## 5 -0.744 -0.479 -0.760 -0.903 -0.537 STAY ## 6 -0.637 -0.479 -0.760 -0.903 -0.200 STAY ## 7 2.50 -0.435 -0.760 -0.903 -0.684 STAY ## 8 5.22 -0.611 -0.937 -1.04 -0.350 MOVE ## 9 4.78 -0.611 -0.937 -1.04 -0.540 MOVE ## 10 5.67 -0.611 -0.937 -1.04 -0.260 MOVE ## # … with 18,941 more rows # create train set, select variables, confirm data types where necessary gps_test &lt;- gps %&gt;% filter(train==0) %&gt;% select(speed,buf100,buf250,buf500,road_dist,move) %&gt;% mutate(road_dist=as.numeric(road_dist)) # drop geometry gps_test &lt;- st_drop_geometry(gps_test) # scale gps_test[1:5] &lt;- lapply(gps_test[1:5], function(x) scale(x)) # inspect gps_test ## # A tibble: 8,068 x 6 ## speed[,1] buf100[,1] buf250[,1] buf500[,1] road_dist[,1] move ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -0.593 -0.557 -0.763 -0.886 -0.317 STAY ## 2 -0.408 -0.512 -0.748 -0.886 0.162 STAY ## 3 -0.448 -0.535 -0.763 -0.886 -0.661 STAY ## 4 -0.570 -0.490 -0.748 -0.886 1.06 STAY ## 5 -0.673 -0.490 -0.748 -0.886 -0.227 STAY ## 6 -0.577 -0.557 -0.748 -0.886 1.48 STAY ## 7 5.66 -0.602 -0.926 -1.03 -0.693 MOVE ## 8 4.18 -0.602 -0.926 -1.03 -0.650 MOVE ## 9 5.14 -0.602 -0.926 -1.03 -0.732 MOVE ## 10 5.26 -0.602 -0.926 -1.00 -0.0660 MOVE ## # … with 8,058 more rows Let’s train our models on our train data and directly predict on our test data. # boosted decision tree with 5 boosting iterations train_boost &lt;- C5.0(as.factor(move) ~., data=gps_train, trials=5) # support vector machine train_svm &lt;- svm(as.factor(move) ~ ., data=gps_train) # random forest with 500 trees, 3 variables at each split, sampling with replacement train_rf &lt;- randomForest(as.factor(move) ~ ., data=gps_train, ntree=500, replace=TRUE, mtry=3) # predict boosted decision tree test_boost &lt;- predict(train_boost, gps_test) # predict support vector machine test_svm &lt;- predict(train_svm, gps_test) # predict random forest test_rf &lt;- predict(train_rf, gps_test) We will use a confusion matrix to inspect our results. A confusion matrix is a table that is often used to describe the performance of a classification model on a set of test data for which the actual values are known. In other words, a confusion matrix can be used to compare the predictions our models make to the ground truth. A confusion matrix basically informs you for all prediction classes how many data points where predicted correctly and how many data points were predicted incorrectly. # create confusion matrices matrix_boost &lt;-table(test_boost, gps_test$move) matrix_svm &lt;- table(test_svm, gps_test$move) matrix_rf &lt;- table(test_rf, gps_test$move) # inspect matrix_boost ## ## test_boost MOVE STAY ## MOVE 3237 104 ## STAY 159 4568 # inspect matrix_svm ## ## test_svm MOVE STAY ## MOVE 3270 112 ## STAY 126 4560 # inspect matrix_rf ## ## test_rf MOVE STAY ## MOVE 3236 84 ## STAY 160 4588 # get overall accuracy boosted decision tree confusionMatrix(matrix_boost)$overall ## Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull ## 0.967402082 0.932983354 0.963292456 0.971169367 0.579077838 ## AccuracyPValue McnemarPValue ## 0.000000000 0.000869138 # get overall accuracy support vector machine confusionMatrix(matrix_svm)$overall ## Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull ## 0.9705007 0.9394538 0.9665725 0.9740840 0.5790778 ## AccuracyPValue McnemarPValue ## 0.0000000 0.3994159 # get overall accuracy random forest confusionMatrix(matrix_rf)$overall ## Accuracy Kappa AccuracyLower AccuracyUpper AccuracyNull ## 9.697571e-01 9.377722e-01 9.657844e-01 9.733854e-01 5.790778e-01 ## AccuracyPValue McnemarPValue ## 0.000000e+00 1.575736e-06 As we can see, all algorithms are highly accurate in classifying our artificial GPS points into moves and stays using the input features that were derived from the raw GPS trajectory data. 7.3.3 Exercise Now we have worked through classifying our points into moves and stays using raw GPS points, there are three further excercises that we want you to do: Add a new set of variables that, for every point in the dataset, contains the number of points within a distance of 100m, 250m, and 500m but for only those points that are within 5 minutes (both sides) of the point under consideration (i.e. use a moving time window to select the points that qualify). So, for instance, for point \\(x\\) 10 other points are within a distance of 100m but only 5 of these points were recorded within 5 minutes of point \\(x\\): we now only want those 5 points and not the full 10 points. Please note: this is not a trivial task and you will have to considerably change the code that you have used so far. Instead of using the move column, use the activity column to classify the GPS points into travel modes (i.e. walk, bike , car) and stays. Incorporate all existing variables as well as the three variables you created in Exercise 1. Assess the relative importance of each of the input variables used in Exercise 2 by permutating (that is shuffling) each input variable and re-running the train and test sequence. So, for instance, shuffle the values within the speed column but leaving all the other values untouched, train the three models, and look at the accuracy and Kappa values to see the new results. Repeat this process for all columns in which every time one of the columns gets permutated but all the other values are untouched. With every iteration you save the accuracy and Kappa values so to get an idea about each variable’s relative importance: the variable that causes the largest drop in accuracy and Kappa values is relatively the most important one. Tip 1: You can shuffle your data by sampling without replacement, e.g. gps_test$speed &lt;- sample(gps_test$speed). Tip 2: Create a function or a loop to conduct this process! 7.4 Take home message Where GPS technology can precisely register locational information, travel characteristics need to be imputed from the data before it can be used by transport researchers and policy makers. As such, throughout the last decade, various methods have been developed for identifying trips, activities, and travel modes from raw GPS trajectories. In this week’s material, we took a closer look at GPS data. We also introduced you to three different discriminative classifiers to classify GPS points into moves and stays. Be aware that we only given you a brief introduction to these classifiers here and there are in fact many different implementations of decision trees as well many ways of parameterising support vector machines (e.g. choice of kernel). It is also important to keep in mind that because the GPS points used in the tutorial were artificially created, the models results in relatively high prediction rates and will be very difficult to transfer to a different context. While this is obviously an issue, the advantage of using artificial data is that parameters and noise levels can be precisely tuned which allow you to systematically compare, test, and develop different algorithms. 7.5 Attributions This week’s content and practical uses content and inspiration from: Van Dijk, J. 2017. Designing Travel Behaviour Change Interventions: a spatiotemporal perspective. Doctoral dissertation. Stellenbosch: Stellenbosch University. [Link] Van Dijk, J. 2018. Identifying activity-travel points from GPS-data with multiple moving windows. Computers, Environment and Urban Systems 70: 84-101. [Link] "],
["unix-tools.html", "8 Unix tools 8.1 Introduction", " 8 Unix tools 8.1 Introduction This week we will be looking at the Unix tools. Basic knowledge of the Unix shell as well as knowing how to retrieve datasets from databases are essential tools in a data scientist’s toolkit. This week you will be introduced to both. In the practical component of this week you will work within the ‘bourne again shell’ (bash) in a command-line environment to create a parallel data processing pipeline involving a database and Unix tools. This week’s content will be made available on 08/03/2021. "],
["web-visualisation.html", "9 Web Visualisation 9.1 Introduction", " 9 Web Visualisation 9.1 Introduction Mapping is at the core of Spatial Data Science. This week we will introduce you to ‘mapping at scale’: how to display large amounts of data with interactive maps through slippy maps. In the practical component we will be using map tiles to create an interactive web map from scratch using JavaScript and Leaflet.js. This week’s content will be made available on 15/03/2021. "],
["a-look-into-the-future.html", "10 A look into the future", " 10 A look into the future This week we will wrap up what we have discussed over the past weeks and take a look at the future of Social and Geographic Data science. This week’s material will be available on Moodle. "]
]
